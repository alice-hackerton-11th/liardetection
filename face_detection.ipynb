{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Blinks, Mouth Size Change, Emotion 측정",
   "id": "bc2a6c5963ddcd9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-24T01:39:13.223913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install mediapipe\n",
    "!pip install opencv-python\n",
    "!pip install deepface\n",
    "!pip insttall numpy"
   ],
   "id": "a5af08c3c102aaac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (0.10.18)\n",
      "Requirement already satisfied: absl-py in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from mediapipe) (24.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from mediapipe) (0.4.35)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from mediapipe) (0.4.35)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from mediapipe) (3.9.2)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from mediapipe) (4.5.5.64)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from mediapipe) (4.25.5)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from mediapipe) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from mediapipe) (0.2.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.4.0 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from jax->mediapipe) (0.5.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.10 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from jax->mediapipe) (1.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from matplotlib->mediapipe) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from matplotlib->mediapipe) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from matplotlib->mediapipe) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from matplotlib->mediapipe) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from matplotlib->mediapipe) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\playdata2\\anaconda3\\envs\\dl_env\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "^C\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "# from google.colab.patches import cv2_imshow\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from deepface import DeepFace\n",
    "\n",
    "# EAR 계산 함수\n",
    "def calculate_ear(landmarks, indices):\n",
    "    # 두 점 간의 유클리드 거리 계산\n",
    "    def euclidean_dist(point1, point2):\n",
    "        return np.linalg.norm(np.array(point1) - np.array(point2))\n",
    "\n",
    "    # EAR 계산 공식\n",
    "    A = euclidean_dist(landmarks[indices[1]], landmarks[indices[5]])\n",
    "    B = euclidean_dist(landmarks[indices[2]], landmarks[indices[4]])\n",
    "    C = euclidean_dist(landmarks[indices[0]], landmarks[indices[3]])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# MAR 계산 함수\n",
    "def mouth_aspect_ratio(mouth_coords):\n",
    "    # 주요 점들을 선택 (입의 상하/좌우 랜드마크)\n",
    "    upper_lip_top = np.array(mouth_coords[2])  # 위쪽 중앙 (mouth_indices[13])\n",
    "    lower_lip_bottom =  np.array(mouth_coords[3])  # 아래쪽 중앙 (mouth_indices[14])\n",
    "    upper_lip_bottom =  np.array(mouth_coords[4])  # 위쪽 안쪽 (mouth_indices[312])\n",
    "    lower_lip_top =  np.array(mouth_coords[5])  # 아래쪽 안쪽 (mouth_indices[82])\n",
    "    left_corner =  np.array(mouth_coords[0])  # 왼쪽 모서리 (mouth_indices[78])\n",
    "    right_corner =  np.array(mouth_coords[1])  # 오른쪽 모서리 (mouth_indices[308])\n",
    "\n",
    "    # 상하 거리 계산\n",
    "    vertical_distance_1 = np.linalg.norm(upper_lip_top - lower_lip_bottom)\n",
    "    vertical_distance_2 = np.linalg.norm(upper_lip_bottom - lower_lip_top)\n",
    "\n",
    "    # 좌우 거리 계산\n",
    "    horizontal_distance = np.linalg.norm(left_corner - right_corner)\n",
    "\n",
    "    # MAR 계산\n",
    "    mar = (vertical_distance_1 + vertical_distance_2) / (2.0 * horizontal_distance)\n",
    "    return mar\n",
    "\n",
    "# 감정 분석 함수\n",
    "def analyze_emotion(face_frame):\n",
    "    try:\n",
    "        analysis = DeepFace.analyze(face_frame, actions=[\"emotion\"], enforce_detection=False)\n",
    "        # print(analysis)  # analysis 출력 추가\n",
    "        return analysis[0][\"dominant_emotion\"]  # 수정된 부분: 리스트 형식으로 반환됨\n",
    "    except Exception as e:\n",
    "        print(f\"DeepFace Error: {e}\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "def extract_facial_features_with_emotion():\n",
    "    # Mediapipe 초기화\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1)\n",
    "\n",
    "    # 비디오 파일 읽기\n",
    "    cap = cv2.VideoCapture('face_example.mp4')\n",
    "\n",
    "    blink_count = 0\n",
    "    ear_threshold = 0.2\n",
    "    ear_consecutive_frames = 2\n",
    "    frame_counter = 0\n",
    "\n",
    "    mouth_size_changes = []\n",
    "    prev_mar = None\n",
    "\n",
    "    emotion_counts = {\"happy\": 0, \"sad\": 0, \"angry\": 0, \"fear\": 0, \"surprise\": 0, \"neutral\": 0, \"disgust\": 0, \"Unknown\": 0}\n",
    "\n",
    "    # EYE_CLOSED_THRESHOLD = 3  # 눈 감은 상태 지속 프레임 임계값\n",
    "    # EAR_THRESHOLD = 0.2  # 눈 종횡비(eye aspect ratio) 임계값\n",
    "    # eye_closed_frames = 0\n",
    "\n",
    "    # frame_count = 0  # 프레임 카운트 변수\n",
    "    # fps = cap.get(cv2.CAP_PROP_FPS)  # FPS 값 얻기\n",
    "    # frame_interval = int(fps)  # 1초에 한 번씩 분석\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # frame_count += 1  # 프레임 카운트 증가\n",
    "\n",
    "        # 이미지 전처리\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "        face_cropped = None\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # 얼굴 랜드마크 그리기\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
    "                    mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=1, circle_radius=1))\n",
    "\n",
    "                # 얼굴 영역 추출 (DeepFace 입력용)\n",
    "                h, w, _ = frame.shape\n",
    "                x_min = y_min = 1\n",
    "                x_max = y_max = 0\n",
    "\n",
    "                # 얼굴 랜드마크의 최소/최대 좌표 계산\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    x_min = min(x_min, x)\n",
    "                    y_min = min(y_min, y)\n",
    "                    x_max = max(x_max, x)\n",
    "                    y_max = max(y_max, y)\n",
    "\n",
    "                # 얼굴 영역 자르기\n",
    "                face_cropped = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "                # 좌표 변환\n",
    "                landmarks = [(int(lm.x * w), int(lm.y * h)) for lm in face_landmarks.landmark]\n",
    "\n",
    "                # EAR 계산 (눈 깜빡임 감지)\n",
    "                left_eye_indices = [33, 160, 158, 133, 153, 144]\n",
    "                right_eye_indices = [362, 385, 387, 263, 373, 380]\n",
    "                left_ear = calculate_ear(landmarks, left_eye_indices)\n",
    "                right_ear = calculate_ear(landmarks, right_eye_indices)\n",
    "                ear = (left_ear + right_ear) / 2.0\n",
    "\n",
    "                # 눈 깜빡임 감지\n",
    "                if ear < ear_threshold:\n",
    "                    frame_counter += 1\n",
    "                else:\n",
    "                    if frame_counter >= ear_consecutive_frames:\n",
    "                        blink_count += 1\n",
    "                    frame_counter = 0\n",
    "\n",
    "                # 입 MAR 계산\n",
    "                mouth_indices = [78, 308, 13, 14, 312, 82]\n",
    "                mouth_coords = [landmarks[i] for i in mouth_indices]\n",
    "                mar = mouth_aspect_ratio(mouth_coords)\n",
    "                if prev_mar is not None:\n",
    "                    mouth_size_changes.append(abs(mar - prev_mar))\n",
    "                prev_mar = mar\n",
    "\n",
    "        # 1초마다 감정 분석\n",
    "        # if frame_count % frame_interval == 0:\n",
    "          # DeepFace 감정 분석\n",
    "        if face_cropped is not None:\n",
    "            emotion = analyze_emotion(face_cropped)\n",
    "            emotion_counts[emotion] += 1\n",
    "            cv2.putText(frame, f\"Emotion: {emotion}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # EAR, 눈 깜빡임, MAR, 감정 상태 출력\n",
    "        cv2.putText(frame, f'EAR: {ear:.2f}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'Blinks: {blink_count}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # 비디오 출력 (1초마다 출력)\n",
    "        # 비디오 출력\n",
    "        cv2.imshow(frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    avg_mouth_size_change = np.mean(mouth_size_changes) if mouth_size_changes else 0\n",
    "    return blink_count, avg_mouth_size_change, emotion_counts\n",
    "\n",
    "# 실행\n",
    "blink_count, avg_mouth_size_change, emotion_counts = extract_facial_features_with_emotion()\n",
    "print(f\"Total Blinks: {blink_count}\")\n",
    "print(f\"Average Mouth Size Change: {avg_mouth_size_change}\")\n",
    "print(f\"Emotion Summary: {emotion_counts}\")"
   ],
   "id": "dccff611a39427fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
